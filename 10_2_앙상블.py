# -*- coding: utf-8 -*-
"""10_2_앙상블.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M8qZUEec7UIT6TjXtLDZPRC8ojyQyvNa
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import torch
import numpy as np
from tqdm import tqdm
import pickle
from transformers import T5ForConditionalGeneration

# 1. 데이터 로드 함수
def load_data(path):
    with open(path, 'rb') as f:
        return pickle.load(f)

# 2. 로짓 추출 함수 (배치 크기 조정 및 메모리 관리)
def extract_t5_logits(model, encodings, device, batch_size=2, save_interval=10):
    """
    T5 모델에서 로짓 값을 추출하는 함수 (시퀀스 길이 고려)
    """
    model.eval()
    all_logits = []
    num_batches = len(encodings) // batch_size + (1 if len(encodings) % batch_size != 0 else 0)

    for i in tqdm(range(num_batches), desc="Extracting logits (T5)", leave=True):
        # 배치 데이터 생성
        batch = encodings[i * batch_size: (i + 1) * batch_size]
        input_ids = torch.stack([item['input_ids'] for item in batch]).to(device)
        attention_mask = torch.stack([item['attention_mask'] for item in batch]).to(device)

        # 디코더 입력 생성
        decoder_input_ids = torch.full(
            (input_ids.shape[0], 1),  # 배치 크기와 길이 1의 디코더 입력
            model.config.decoder_start_token_id,
            dtype=torch.long,
            device=device
        )

        # 로짓 추출
        with torch.no_grad():
            outputs = model(input_ids=input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids)
            logits = outputs.logits.cpu().numpy()

            # T5는 (batch_size, seq_len, vocab_size) 형태로 로짓을 반환
            # 시퀀스 길이를 고려하여 마지막 시점의 로짓만 사용
            logits = logits[:, -1, :]  # 마지막 시점의 로짓만 사용

            all_logits.append(logits)

        # GPU 메모리 해제
        torch.cuda.empty_cache()

        # 중간 저장: 일정 배치마다 저장 (예시: 10개 배치마다 저장)
        if len(all_logits) >= save_interval:
            batch_logits = np.concatenate(all_logits, axis=0)
            save_to_drive(batch_logits, t5_val_logits_path)
            all_logits = []  # 메모리 해제를 위해 리스트 비우기

    # 남은 로짓 저장
    if all_logits:
        batch_logits = np.concatenate(all_logits, axis=0)
        save_to_drive(batch_logits, t5_val_logits_path)

    return np.concatenate(all_logits, axis=0)

# 3. 데이터 저장 함수
def save_to_drive(data, path):
    """
    데이터를 지정된 경로에 pickle 형식으로 저장합니다.
    경로에 폴더가 없으면 자동으로 생성합니다.
    """
    directory = os.path.dirname(path)
    if not os.path.exists(directory):
        os.makedirs(directory)
        print(f"Directory created: {directory}")

    with open(path, 'wb') as f:
        pickle.dump(data, f)
    print(f"Data saved to {path}")

# 4. GPU 설정
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# 5. 데이터 로드 (여기서는 예시로 val_encodings_t5만 로드)
val_encodings_t5 = load_data('/content/drive/MyDrive/LLM/tokenizing/val_encodings_t5.pkl')

# 6. 모델 로드
t5_model = T5ForConditionalGeneration.from_pretrained(
    '/content/drive/MyDrive/LLM/t5_model/checkpoint-1000').to(device)

# 7. 로짓 추출 및 저장 경로
t5_val_logits_path = '/content/drive/MyDrive/LLM/logits-확률평균/t5_logits2_val.pkl'

# 8. 로짓 추출 및 저장
print("Extracting logits from T5...")
t5_logits_val = extract_t5_logits(t5_model, val_encodings_t5, device, batch_size=2)  # 배치 크기를 2로 설정

# 최종적으로 남은 로짓들을 저장
save_to_drive(t5_logits_val, t5_val_logits_path)

"""스태킹앙상블"""

import pickle

# 데이터 로드 함수
def load_data(path):
    with open(path, 'rb') as f:
        return pickle.load(f)

# 학습 및 검증 데이터 경로
train_encodings_bert = load_data('/content/drive/MyDrive/LLM/tokenizing/train_encodings_bert.pkl')
train_encodings_roberta = load_data('/content/drive/MyDrive/LLM/tokenizing/train_encodings_roberta.pkl')
val_encodings_bert = load_data('/content/drive/MyDrive/LLM/tokenizing/val_encodings_bert.pkl')
val_encodings_roberta = load_data('/content/drive/MyDrive/LLM/tokenizing/val_encodings_roberta.pkl')

# 로드된 데이터 확인
print(f"Train BERT Encodings: {len(train_encodings_bert)} samples")
print(f"Train RoBERTa Encodings: {len(train_encodings_roberta)} samples")
print(f"Validation BERT Encodings: {len(val_encodings_bert)} samples")
print(f"Validation RoBERTa Encodings: {len(val_encodings_roberta)} samples")

import pickle
import numpy as np
from sklearn.model_selection import train_test_split

# 로짓 데이터 불러오기 함수
def load_logits(file_path):
    with open(file_path, 'rb') as f:
        return pickle.load(f)

# 로짓 경로 설정
bert_logits_path = '/content/drive/MyDrive/LLM/logits-확률평균/bert_logits_val.pkl'
roberta_logits_path = '/content/drive/MyDrive/LLM/logits-확률평균/roberta_logits_val.pkl'

# 로짓 데이터 불러오기
bert_logits = load_logits(bert_logits_path)
roberta_logits = load_logits(roberta_logits_path)

# 로짓 크기 출력
print("BERT logits shape:", bert_logits.shape)
print("RoBERTa logits shape:", roberta_logits.shape)

# 검증용 레이블 정의 (여기서는 예시로 random 값 사용)
val_labels = np.random.randint(0, 2, size=bert_logits.shape[0])  # 이 부분은 실제 val_labels로 교체해야 합니다.

# 배치 크기와 로짓 차원 확인
assert bert_logits.shape[0] == roberta_logits.shape[0], "배치 크기가 일치하지 않습니다!"

# 로짓 결합 (BERT와 RoBERTa 로짓만 사용)
X = np.hstack([bert_logits, roberta_logits])
y = np.array(val_labels)  # y는 별도로 정의되어야 합니다.

# 훈련/검증 데이터 나누기
X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)

# 이제 X_train, X_valid, y_train, y_valid를 사용하여 모델을 훈련할 수 있습니다.

from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix

"""스태킹 앙상블"""

import pickle
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# 데이터 로드 함수
def load_data(path):
    with open(path, 'rb') as f:
        return pickle.load(f)

# 로짓 데이터 불러오기 함수
def load_logits(file_path):
    with open(file_path, 'rb') as f:
        return pickle.load(f)

# 로짓 경로 설정
bert_logits_path = '/content/drive/MyDrive/LLM/logits-확률평균/bert_logits_val.pkl'
roberta_logits_path = '/content/drive/MyDrive/LLM/logits-확률평균/roberta_logits_val.pkl'

# 레이블 경로 설정
train_labels_path = '/content/drive/MyDrive/LLM/data/train_labels.pkl'
val_labels_path = '/content/drive/MyDrive/LLM/data/val_labels.pkl'

# 로짓 데이터 불러오기
bert_logits = load_logits(bert_logits_path)
roberta_logits = load_logits(roberta_logits_path)

# 레이블 데이터 불러오기
train_labels = load_data(train_labels_path)
val_labels = load_data(val_labels_path)


# 배치 크기와 로짓 차원 확인
assert bert_logits.shape[0] == roberta_logits.shape[0], "배치 크기가 일치하지 않습니다!"

# 로짓 결합 (BERT와 RoBERTa 로짓만 사용)
X = np.hstack([bert_logits, roberta_logits])
y = np.array(val_labels)  # y는 검증 데이터 레이블

# 훈련/검증 데이터 나누기
X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)

# 앙상블 모델 훈련 (예시로 LogisticRegression 사용)
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

# 예측
predictions = model.predict(X_valid)

# 성능 평가
precision = precision_score(y_valid, predictions)
recall = recall_score(y_valid, predictions)
f1 = f1_score(y_valid, predictions)
cm = confusion_matrix(y_valid, predictions)

print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1-score: {f1}")
print(f"Confusion Matrix:\n{cm}")

"""확률평균 앙상블"""

import numpy as np
from scipy.special import softmax
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# 각 모델의 로짓 (BERT, RoBERTa)
bert_probs = softmax(bert_logits, axis=1)
roberta_probs = softmax(roberta_logits, axis=1)

# 확률 평균 모델 (BERT, RoBERTa의 확률 평균)
average_probs = (bert_probs + roberta_probs) / 2

# 최종 예측: 확률이 더 높은 클래스를 선택
final_predictions = np.argmax(average_probs, axis=1)

# 정확도 계산
accuracy = accuracy_score(val_labels, final_predictions)
print(f"Ensemble model accuracy (probability averaging): {accuracy}")

# 성능 평가
precision = precision_score(val_labels, final_predictions, average='weighted')  # 다중 클래스에서 weighted 평균 사용
recall = recall_score(val_labels, final_predictions, average='weighted')
f1 = f1_score(val_labels, final_predictions, average='weighted')
cm = confusion_matrix(val_labels, final_predictions)

print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1-score: {f1}")
print(f"Confusion Matrix:\n{cm}")

"""랜덤포레스트 앙상블"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.model_selection import train_test_split
import numpy as np

# 로짓을 결합한 X와 레이블 y
X = np.hstack([bert_logits, roberta_logits])
y = np.array(val_labels)

# 훈련/검증 데이터 나누기
X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)

# 랜덤 포레스트 모델 학습
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# 예측
rf_predictions = rf_model.predict(X_valid)

# 정확도 계산
accuracy = accuracy_score(y_valid, rf_predictions)
print(f"Random Forest accuracy: {accuracy}")

# 성능 평가 (정밀도, 재현율, F1-score, 혼동 행렬)
precision = precision_score(y_valid, rf_predictions, average='weighted')  # 다중 클래스에서 weighted 평균 사용
recall = recall_score(y_valid, rf_predictions, average='weighted')
f1 = f1_score(y_valid, rf_predictions, average='weighted')
cm = confusion_matrix(y_valid, rf_predictions)

print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1-score: {f1}")
print(f"Confusion Matrix:\n{cm}")

"""XGBOOST 앙상블"""

import xgboost as xgb
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.model_selection import train_test_split
import numpy as np

# 로짓을 결합한 X와 레이블 y
X = np.hstack([bert_logits, roberta_logits])
y = np.array(val_labels)

# 훈련/검증 데이터 나누기
X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)

# XGBoost 모델 학습
xgb_model = xgb.XGBClassifier(n_estimators=100, random_state=42)
xgb_model.fit(X_train, y_train)

# 예측
xgb_predictions = xgb_model.predict(X_valid)

# 정확도 계산
accuracy = accuracy_score(y_valid, xgb_predictions)
print(f"XGBoost accuracy: {accuracy}")

# 성능 평가 (정밀도, 재현율, F1-score, 혼동 행렬)
precision = precision_score(y_valid, xgb_predictions, average='weighted')  # 다중 클래스에서 weighted 평균 사용
recall = recall_score(y_valid, xgb_predictions, average='weighted')
f1 = f1_score(y_valid, xgb_predictions, average='weighted')
cm = confusion_matrix(y_valid, xgb_predictions)

print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1-score: {f1}")
print(f"Confusion Matrix:\n{cm}")

"""최적의 앙상블 모델 저장"""

import joblib  # 모델 저장을 위한 joblib 라이브러리

# 모델 저장 (joblib 사용)
model_save_path = '/content/drive/MyDrive/LLM/random_forest_model.joblib'
joblib.dump(rf_model, model_save_path)
print(f"Model saved to {model_save_path}")