{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "mount_file_id": "1zHNBWZDxjaDM6kLN-h0lVVlyD3MfRklw",
      "authorship_tag": "ABX9TyPwlzQkBfMuFBznE7vhSbQx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Onedory/DDos-detection-LLM/blob/main/10_1_%ED%86%A0%ED%81%AC%EB%82%98%EC%9D%B4%EC%A0%80_%EC%A0%80%EC%9E%A5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxYVwoI3plUk",
        "outputId": "caae5219-6165-4119-ca51-e460ac2b526a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 2264594 training texts and 566149 validation texts.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2077: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "Tokenizing T5 Train: 100%|██████████| 2264594/2264594 [02:09<00:00, 17437.48texts/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized encodings saved successfully in /content/drive/MyDrive/LLM/tokenizing/\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "from transformers import BertTokenizer, RobertaTokenizer, T5Tokenizer\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from huggingface_hub import login\n",
        "\n",
        "# 1. Hugging Face Hub 로그인 (선택 사항: 인증을 원하는 경우)\n",
        "# login(\"your_huggingface_token\")\n",
        "\n",
        "# 2. 저장된 데이터셋 로드\n",
        "data_dir = '/content/drive/MyDrive/LLM/data'  # 데이터셋 저장 경로\n",
        "with open(os.path.join(data_dir, 'train_texts.pkl'), 'rb') as f:\n",
        "    train_texts = pickle.load(f)\n",
        "\n",
        "with open(os.path.join(data_dir, 'val_texts.pkl'), 'rb') as f:\n",
        "    val_texts = pickle.load(f)\n",
        "\n",
        "print(f\"Loaded {len(train_texts)} training texts and {len(val_texts)} validation texts.\")\n",
        "\n",
        "# 3. 저장 경로 설정\n",
        "save_dir = '/content/drive/MyDrive/LLM/tokenizing'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# 4. 토크나이저 로드\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", use_auth_token=False)\n",
        "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\", use_auth_token=False)\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\", legacy=False, use_auth_token=False)\n",
        "\n",
        "\n",
        "# 5. 데이터 토크나이징 함수 (개별 텍스트 처리)\n",
        "def tokenize_text(tokenizer, text):\n",
        "    encoding = tokenizer(text, truncation=True, padding=\"max_length\", max_length=128, return_tensors=\"pt\")\n",
        "    return {key: val.squeeze(0) for key, val in encoding.items()}  # 차원 축소\n",
        "\n",
        "# 6. 병렬 토크나이징 함수\n",
        "def parallel_tokenize_data(tokenizer, texts, desc, num_workers=4):\n",
        "    encodings = []\n",
        "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "        results = list(tqdm(executor.map(lambda x: tokenize_text(tokenizer, x), texts), desc=desc, unit=\"texts\", total=len(texts)))\n",
        "        encodings.extend(results)\n",
        "    return encodings\n",
        "\n",
        "# 8. 토크나이징 결과 저장 함수\n",
        "def save_encodings(encodings, filename):\n",
        "    with open(os.path.join(save_dir, filename), 'wb') as f:\n",
        "        pickle.dump(encodings, f)\n",
        "\n",
        "# 7. 학습 및 검증 데이터 토크나이징\n",
        "\n",
        "train_encodings_bert = parallel_tokenize_data(bert_tokenizer, train_texts, \"Tokenizing BERT Train\", num_workers=8)\n",
        "val_encodings_bert = parallel_tokenize_data(bert_tokenizer, val_texts, \"Tokenizing BERT Validation\", num_workers=8)\n",
        "save_encodings(train_encodings_bert, \"train_encodings_bert.pkl\")\n",
        "save_encodings(val_encodings_bert, \"val_encodings_bert.pkl\")\n",
        "\n",
        "train_encodings_roberta = parallel_tokenize_data(roberta_tokenizer, train_texts, \"Tokenizing RoBERTa Train\", num_workers=8)\n",
        "val_encodings_roberta = parallel_tokenize_data(roberta_tokenizer, val_texts, \"Tokenizing RoBERTa Validation\", num_workers=8)\n",
        "save_encodings(train_encodings_roberta, \"train_encodings_roberta.pkl\")\n",
        "save_encodings(val_encodings_roberta, \"val_encodings_roberta.pkl\")\n",
        "\n",
        "train_encodings_t5 = parallel_tokenize_data(t5_tokenizer, train_texts, \"Tokenizing T5 Train\", num_workers=8)\n",
        "val_encodings_t5 = parallel_tokenize_data(t5_tokenizer, val_texts, \"Tokenizing T5 Validation\", num_workers=8)\n",
        "save_encodings(train_encodings_t5, \"train_encodings_t5.pkl\")\n",
        "save_encodings(val_encodings_t5, \"val_encodings_t5.pkl\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"Tokenized encodings saved successfully in /content/drive/MyDrive/LLM/tokenizing/\")\n"
      ]
    }
  ]
}